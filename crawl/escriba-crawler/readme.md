
# Escriba transcript downloader

This folder contains a small script and associated data for downloading speech
transcripts from **Escriba**, an online service operated by the Brazilian fact‑checking
agency **Aos Fatos**.  These transcripts were an important component of the
**BOL4Y** dataset. These Escriba transcripts are already segmented and therefore bypass the
Whisper transcription step used on the other videos.

## Repository contents

```
escriba-crawler/
├── escriba.py    # Python script that uses Playwright to download transcripts
└── data/         # Transcripts downloaded from Escriba as CSV files
```

### `escriba.py`

This script automates the download of CSV transcripts from the
`https://escriba.aosfatos.org/` portal.  It uses the
[Playwright](https://playwright.dev/python/) library to open each Escriba page,
trigger the “Export CSV” button and save the resulting transcript.
The script reads a list of pages from `../dump.csv`, which is generated by
the `crawl/aos-fatos` pipeline.  Each row contains a fact‑check record
(`fact_check_id`) and the URL of the corresponding Escriba page (`origem_links`).
For each entry that has not already been downloaded, the script calls
`download_page(url, page, fact_check_id)` to fetch the transcript and
save it as `<page>-<fact_check_id>.csv` in the working directory.  A
simple sleep is used to avoid overloading the server.

To run the script you need Python 3, [Pandas](https://pandas.pydata.org/)
and Playwright.  Install dependencies and the browser runtime via:

```bash
pip install pandas playwright
playwright install  # install Chromium for Playwright
```

Then invoke the downloader from the `escriba-crawler` folder:

```bash
cd crawl/escriba-crawler
python escriba.py
```

The script will read `../dump.csv` and download all transcripts referenced
there.  By default files are saved in the current directory; after running
the script you can move them into the `data/` folder for long‑term storage.

### `data/` directory

The `data` directory houses the raw CSV files downloaded via `escriba.py`.
Each file corresponds to a single transcript and is named `<page>-<fact_check_id>.csv`.
The CSVs come directly from the Escriba service and typically contain the
following columns (exact schema may vary slightly across downloads):

| column              | description                                                      |
|--------------------|------------------------------------------------------------------|
| `start_time`       | Start time of the utterance (in seconds)                         |
| `end_time`         | End time of the utterance (in seconds)                           |
| `speaker`          | Speaker label (if provided)                                      |
| `text`             | Transcribed utterance text                                       |

These transcripts are already segmented by Escriba, so no further
segmentation is necessary.  In our paper we refer to
them as *pre‑segmented transcripts*.  When building the dataset we
merged the segments corresponding to each fact‑checked claim into a single
sample; 2,355 such positive examples were obtained in the BOL4Y corpus and
78 in EI22.

## Relation to the paper

The transcripts collected via this script form part of the BOL4Y dataset
released with the paper.
